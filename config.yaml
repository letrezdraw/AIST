# AIST Configuration File
#
# Copy this file to 'config.yaml' in the project root and edit it to your needs.
# The application will load 'config.yaml' at startup.

assistant:
  name: "AIST"
  # Phrases that will activate the assistant from the DORMANT state.
  # The STT engine is highly optimized to listen for these specific phrases.
  activation_phrases:
    - "hey assist"
    - "okay assist"
    - "hey assistant"
    - "okay assistant"
  # Phrases that will shut down the assistant from any state.
  exit_phrases:
    - "assist exit"
    - "assist quit"
  # Phrases that will return the assistant to the DORMANT state.
  deactivation_phrases:
    - "assist pause"
    - "go to sleep"
  # How similar (in percent) speech must be to a command phrase to be a match.
  # Lower is more lenient but risks more false positives. 85 is a good starting point.
  fuzzy_match_threshold: 85
  # The maximum time in seconds a skill is allowed to run before being terminated.
  skill_timeout: 5
  # The number of user/assistant exchanges to keep in short-term memory for context.
  conversation_history_length: 5

ipc:
  # Port for the event bus, broadcasting state changes to GUIs or other listeners.
  # This is separate from the main command/response channel.
  event_bus_port: 5556
  # Port for receiving text commands from the GUI or other text-based clients.
  text_command_port: 5557
  # Port for broadcasting log records to the GUI or other listeners.
  log_broadcast_port: 5558

models:
  llm:
    path: "data/models/llm/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
    gpu_layers: 99
    context_length: 4096
    max_new_tokens: 150
  tts:
    # The TTS provider to use. 'piper' is the default high-quality local engine.
    provider: "piper"
    # --- Piper Provider Settings ---
    piper_voice_model: "data/models/tts/en_US-lessac-medium.onnx"
  stt:
    vosk_model_path: "data/models/stt/vosk-model-en-us-0.22"
    # The STT provider to use. 'vosk' is the default lightweight engine.
    # 'whisper' will provide higher quality recognition.
    provider: "whisper"
    # --- Whisper Provider Settings ---
    # Model size (e.g., "tiny.en", "base.en", "small.en", "medium.en"). Larger models are
    # more accurate but slower and use more memory. ".en" models are English-only.
    whisper_model_name: "base.en"
    whisper_device: "cuda" # "cuda" for NVIDIA GPUs, "cpu" for CPU

audio:
  stt:
    # Confidence threshold (0.0 to 1.0) for accepting a transcription.
    # Lower values are more permissive but may result in more errors.
    confidence_threshold: 0.85

logging:
  # Path for the log folder, relative to project root.
  folder: "data/logs"
  # Whether to show log output in the console. Set to false for a cleaner terminal.
  console_enabled: false

hotkeys:
  # Global hotkey to force quit the application.
  quit: "ctrl+win+x"